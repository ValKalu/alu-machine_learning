# -*- coding: utf-8 -*-
"""air_quality_forecasting_starter_code (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/ValKalu/alu-machine_learning/blob/main/supervised_learning/Beijing%20air%20quality%20forecast/air_quality_forecasting_starter_code%20(1).ipynb

# Beijing Air Quality Forecasting Starter Notebook
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import tensorflow as tf

# Mount Google Drive to access datasets
from google.colab import drive
drive.mount('/content/drive')

# Load the datasets
# Ensure train.csv and test.csv are saved in your Google Drive in the same folder.
# Replace the file paths below with the actual paths to your dataset.
train = pd.read_csv('/content/drive/MyDrive/content/train.csv')
test = pd.read_csv('/content/drive/MyDrive/content/test.csv')

"""# Explore the training data

In this sections explore your dataset with appropiate statistics and visualisations to understand your better. Ensure that you explain output of every code cell and what it entails.
"""

# Inspecting the first few rows of the dataset to understand its structure.
print("Training Data Overview:")
train.head()



train.columns

# Ensure 'datetime' column is in datetime format
train['datetime'] = pd.to_datetime(train['datetime'])

test['datetime'] = pd.to_datetime(test['datetime'])

# Set the 'datetime' column as the index for better time-series handling
train.set_index('datetime', inplace=True)
# val.set_index('datetime', inplace=True)
test.set_index('datetime', inplace=True)

"""# Handle missing values


- Check the dataset for missing values and decide how to handle them.
- In this example, missing values are filled with the mean. You can experiment with other strategies.
"""

train.fillna(train.mean(numeric_only=True), inplace=True)
test.fillna(test.mean(numeric_only=True), inplace=True)

"""# Separate features and target

- Feel free to trop any non-essential columns like that you think might not contribute to modeling.
"""

X_train = train.drop(['pm2.5', 'No'], axis=1)
y_train = train['pm2.5']
X_test = test.drop(['No'], axis=1)

# Reshape data for LSTM input
# LSTM models require data in the shape (samples, timesteps, features).
# Here, the data is reshaped to add a "timesteps" dimension.
X_train = np.expand_dims(X_train, axis=1)
# createing sequences for LSTM
def create_sequences(data, target_column=None, timesteps=1):
    X, y = [], []
    # 'No' column when present not needed for feature or target
    data_for_seq = data.drop('No', axis=1, errors='ignore')

    if target_column:
        features = data_for_seq.drop(target_column, axis=1)
        target = data_for_seq[target_column]
    else: # test data unknown target
        features = data_for_seq
        target = None

    for i in range(len(features) - timesteps):
        X.append(features.iloc[i:(i + timesteps)].values)
        if target is not None:
            y.append(target.iloc[i + timesteps]) # Predict the value at the next timestep
    if target is not None:
        return np.array(X), np.array(y)
    else:
        return np.array(X)

# The number of timesteps back 5 hours
TIMESTEPS = 5 # 5,experimenting with 1, 3, 10.

# training data sequences
X_train_sequences, y_train_sequences = create_sequences(train, 'pm2.5', TIMESTEPS)
print(f"\nShape of X_train_sequences: {X_train_sequences.shape}")
print(f"Shape of y_train_sequences: {y_train_sequences.shape}")

# Createing sequences for test data
X_test_sequences = create_sequences(test, target_column=None, timesteps=TIMESTEPS)
print(f"Shape of X_test_sequences: {X_test_sequences.shape}")


# Model Design and Training

# prevent conflicts
tf.keras.backend.clear_session()

"""# Build model

Below is a simple LSTM model. Your task is to experiment with different parameters like, numbers of layers, units, activation functions, and optimizers, etc to get the best performing model. Experiment with other optimizers (e.g., SGD) or hyperparameters to improve performance.
"""

# define model
model = Sequential([
    LSTM(32, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(1)
])
#Define model
model = Sequential([
    LSTM(64, activation='relu', input_shape=(X_train_sequences.shape[1], X_train_sequences.shape[2])),
    Dense(1)
])
#Import the Adam optimizer
from tensorflow.keras.optimizers import Adam

# Compile the model
model.compile(
    optimizer=Adam(learning_rate= 0.001),
    loss='mse',
    metrics=[lambda y, y_pred: tf.sqrt(tf.reduce_mean(tf.square(y - y_pred)))]  # RMSE metric
)
#Define model
model = Sequential([
    LSTM(32, activation='relu', input_shape=(X_train_sequences.shape[1], X_train_sequences.shape[2])),
    Dense(1)
])
#Define model
model = Sequential([
    LSTM(32, activation='relu', input_shape=(X_train_sequences.shape[1], X_train_sequences.shape[2])),
    Dense(1)
])
#Compile model
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse',
    metrics=[lambda y, y_pred: tf.sqrt(tf.reduce_mean(tf.square(y - y_pred)))]  # RMSE metric
)
# Display the model architecture
model.summary()

#Display the model architecture
model.summary()
model.summary()

# Train the model
# Adjust epochs and batch_size based on your experiments
EPOCHS = 20 # Increase epochs for better learning
BATCH_SIZE = 64 # Experiment with different batch sizes

print(f"\nStarting model training with {EPOCHS} epochs and batch size {BATCH_SIZE}...")
history = model.fit(
    X_train_sequences, y_train_sequences, # <--- Use the sequence-created data here
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1 # Show progress during training
)

# Calculate training loss
train_predictions = model.predict(X_train)
train_loss = np.mean((y_train - train_predictions.flatten())**2)

# Plot training loss
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')  # Training loss during epochs
plt.axhline(y=train_loss, color='blue', linestyle='--', label='Final rain Loss')  # Final training loss
plt.title('Loss on Training Data')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.show()

print(f"Final Training Loss (MSE): {train_loss}")

# Prepare the test data
X_test = test.drop(['No'], axis=1)
X_test = np.expand_dims(X_test, axis=1)

# Make predictions on the test set using trained model to predict "pm2.5" concentrations
predictions = model.predict(X_test)

# Ensure predictions do not contain NaN values
predictions = np.nan_to_num(predictions)

# Convert predictions to integers
predictions = np.round(predictions).astype(int)

# Prepare the submission file
# Convert 'row ID' index to string and remove leading zeros for single-digit hours
submission = pd.DataFrame({
    'row ID': pd.to_datetime(test.index).strftime('%Y-%m-%d %-H:%M:%S'),  # Remove leading zeros for hours
    'pm2.5': predictions.flatten()
})

# Sort the submission by 'row ID' to match the solution file exactly
submission = submission.sort_values(by='row ID')

# Save the file in CSV format for submission on Kaggle
submission.to_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forcasting/subm_fixed.csv', index=False)