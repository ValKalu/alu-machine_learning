{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"ml_pipeline_Avit_Brian_Mugisha_music_data_preprocessing.ipynb\n",
    "\n",
    "# Data Processing Approach for Portfolio Project\n",
    "\n",
    "## Project Title: Music Mastery (MM)\n",
    "\n",
    "## Student Name: Avit Brian MUGISHA\n",
    "\n",
    "---\n",
    "\n",
    "1. **Data Sources and Aggregation:**\n",
    "   - List all sources of data for the project. **You must consider sources outside Kaggle, Google datasets** (insert links where necessary to online platforms, research papers, etc.)\n",
    "\n",
    "   - **Data Sources:** The dataset used for this project is sourced from the Music Data Collection [Insert link to dataset if available].\n",
    "\n",
    "   - Determine if data aggregation from multiple sources is necessary for comprehensive analysis.\n",
    "\n",
    "   - **Data Aggregation:** The dataset provides comprehensive details about artists, albums, and tracks. For this project, I will only use this single source as it covers a wide range of relevant features.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'summatives/talentai/data/talentai_artist_records.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Summary statistics\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. **Data Format Transformation:**\n",
    "   - Describe the current format of the data.\n",
    "   - Outline the planned transformation to a unified format suitable for analysis and modeling.\n",
    "\n",
    "    - **Current Format:** The dataset is structured in a tabular format with features such as artist name, genre, popularity, and number of tracks.\n",
    "    - **Transformation Plan:** The data will be transformed into a format suitable for analysis by standardizing numerical features and encoding categorical variables.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target (if applicable)\n",
    "X = df.drop('target_column', axis=1)  # Replace 'target_column' with your actual target column if applicable\n",
    "y = df['target_column']  # Replace 'target_column' with your actual target column if applicable\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Apply scaling to numerical columns\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "X_processed = X\n",
    "X_processed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. **Data Exploration:**\n",
    "   - Enumerate the features included in the dataset.\n",
    "   \n",
    "   - Summarize findings from exploratory data analysis (EDA) including distributions, correlations, and outliers.\n",
    "    - **Features:** Features included in the dataset are: artist_name, genre, popularity, number_of_tracks, etc.\n",
    "  \n",
    "    - **Findings:** Conduct exploratory data analysis (EDA) to understand distributions, correlations, and outliers in the dataset. Features such as popularity and genre will be analyzed to gain insights into their relationship with the target variable (if applicable). \n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Selecting features for EDA\n",
    "selected_features = ['popularity', 'number_of_tracks']\n",
    "\n",
    "# Plotting distributions for selected features\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, feature in enumerate(selected_features, start=1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.histplot(data=df, x=feature, kde=True, bins=20, alpha=0.7, palette='viridis')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "4. **Hypothesis Testing:**\n",
    "   - State any preexisting hypotheses about the data.\n",
    "   - Explain methodologies to empirically test these hypotheses.\n",
    "\n",
    "    - **Hypothesis 1:** Popularity is positively correlated with the number of tracks an artist has.\n",
    "    - **Hypothesis 2:** Genre distribution impacts the overall popularity of the artist.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. **Handling Sparse/Dense Data and Outliers:**\n",
    "   - Assess the density of the data.\n",
    "   - Propose strategies to handle missing data and outliers while maintaining dataset integrity.\n",
    "\n",
    "    - **Data Density:** Assess the density of missing data.\n",
    "    - **Strategies:** Implement strategies such as mean imputation for missing values and IQR method for outlier detection.\n",
    "\"\"\"\n",
    "\n",
    "# Calculate quantiles for capping\n",
    "popularity_cap = df['popularity'].quantile(0.99)\n",
    "\n",
    "# Cap extreme values for selected features\n",
    "X_processed['popularity'] = X_processed['popularity'].clip(upper=popularity_cap)\n",
    "\n",
    "# Impute missing values with mean\n",
    "X_processed = X_processed.fillna(X_processed.mean())\n",
    "\n",
    "X_processed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. **Data Splitting:**\n",
    "   - Define a methodology to split the dataset into training, validation, and testing sets.\n",
    "   - Ensure randomness and representativeness in each subset.\n",
    "    - **Methodology:** Split the dataset into training, validation, and testing sets.\n",
    "    - **Considerations:** Ensure randomness and stratification based on the target variable for representativeness (if applicable).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_processed\n",
    "y = df['target_column']  # Replace 'target_column' with your actual target column if applicable\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. **Bias Mitigation:**\n",
    "   - Implement techniques to identify and mitigate biases in the dataset.\n",
    "   - Ensure fairness and equity in data representation.\n",
    "    - **Techniques:** SMOTE was used to address class imbalance in the target variable (if applicable).\n",
    "\"\"\"\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to address class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled class distribution:\", y_train_resampled.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "8. **Features for Model Training:**\n",
    "   - Identify relevant features for training the model.\n",
    "   - Rank features based on their significance to project objectives.\n",
    "\n",
    " **Your answer for features must be plotted/ show your working code-wise **\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh', figsize=(8, 5))\n",
    "plt.title('Top 10 Important Features')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "9. **Types of Data Handling:**\n",
    "   - Classify the types of data (categorical, numerical, etc.) present in the dataset.\n",
    "   - Plan preprocessing steps for each data type.\n",
    "\n",
    "    - **Data Types:** The dataset includes numerical and categorical data.\n",
    "\n",
    "      - Numerical Columns:\n",
    "\n",
    "          popularity,\n",
    "          number_of_tracks,\n",
    "      - Categorical Columns:\n",
    "\n",
    "          genre,\n",
    "          artist_name,\n",
    "    - **Preprocessing:** Plan preprocessing steps such as encoding categorical variables and scaling numerical features.\n",
    "\"\"\"\n",
    "\n",
    "# Example preprocessing pipeline for numerical and categorical features\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)  # Define categorical_cols appropriately\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "10. **Data Transformation for Modeling:**\n",
    "    - Specify methods for transforming raw data into a model-friendly format.\n",
    "    - Detail steps for normalization, scaling, or encoding categorical variables.\n",
    "\n",
    "      - **Methods:** Normalize numerical features and encode categorical variables for model training.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "11. **Data Storage:**\n",
    "    - Determine where and how processed data will be stored.\n",
    "    - Choose suitable storage solutions ensuring accessibility and security.\n",
    "      - **Storage Solution:** Store processed data in a structured format such as CSV files or a database for accessibility and security.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talentai",
   "language": "python",
   "name": "talentai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
